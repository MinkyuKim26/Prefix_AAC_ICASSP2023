{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82fed595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "# custom\n",
    "from util import *\n",
    "from AAC_Prefix.AAC_Prefix import * # network\n",
    "from Train import *\n",
    "    \n",
    "TEST_BATCH_SIZE = 5\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available() \n",
    "device = torch.device('cuda:0' if USE_CUDA else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71188d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cuai5th/anaconda3/envs/minkyu_env/lib/python3.8/site-packages/torchlibrosa/stft.py:193: FutureWarning: Pass size=512 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  fft_window = librosa.util.pad_center(fft_window, n_fft)\n",
      "/home/cuai5th/anaconda3/envs/minkyu_env/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use Custom Tokenizer\n",
      "temporal feature's mapping network : num_head = 8 num_layers = 4\n",
      "global feature ver's mapping network : num_head = 8 num_layers = 4\n",
      "Encoder freezing\n",
      "GPT2 freezing\n"
     ]
    }
   ],
   "source": [
    "# table_num = 1 : Evaluation on Clotho\n",
    "# table_num = 2 : Evaluation on AudioCaps\n",
    "\n",
    "# setting_num = 1 : train dataset == test dataset\n",
    "# setting_num = 2 : train dataset != test dataset\n",
    "# setting_num = 3 : overall datasets(Clotho & AudioCaps) <- need to test by using compressed audio\n",
    "\n",
    "table_num = 1\n",
    "setting_num = 1\n",
    "\n",
    "if setting_num == 3 :\n",
    "    is_settingnum_3 = True\n",
    "else : \n",
    "    is_settingnum_3 = False\n",
    "\n",
    "model = get_model_in_table(table_num, setting_num, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d414ad18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "get dataset...: 100%|███████████████████████| 960/960 [00:00<00:00, 1407.96it/s]\n",
      "get dataset...: 100%|██████████████████████| 1045/1045 [00:06<00:00, 166.82it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataloader_audiocaps  = CreateDataloader(None, './AudioCaps', TEST_BATCH_SIZE, 'test', None, is_TrainDataset = False, tokenizer_type = None, is_settingnum_3 = is_settingnum_3)\n",
    "test_dataloader_clotho = CreateDataloader(None, './Clotho', TEST_BATCH_SIZE, 'evaluation', None, is_TrainDataset = False, tokenizer_type = None, is_settingnum_3 = is_settingnum_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9ac714b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval using dataset...: 100%|████████████████| 1045/1045 [03:32<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.006926\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 70696 tokens at 477904.72 tokens per second.\n",
      "PTBTokenizer tokenized 11930 tokens at 143140.25 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 9840, 'reflen': 10288, 'guess': [9840, 8795, 7750, 6705], 'correct': [5772, 2326, 925, 286]}\n",
      "ratio: 0.9564541213062834\n",
      "Bleu_1: 0.560\n",
      "Bleu_2: 0.376\n",
      "Bleu_3: 0.253\n",
      "Bleu_4: 0.160\n",
      "computing METEOR score...\n",
      "METEOR: 0.170\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.378\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.392\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.9 sec].\n",
      "Threads( StanfordCoreNLP ) [2.753 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 10.78 s\n",
      "SPICE: 0.118\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.255\n"
     ]
    }
   ],
   "source": [
    "if table_num == 1 or setting_num == 3 :\n",
    "    metrics_clotho, captions_pred_clotho, captions_gt_clotho = eval_model(model, test_dataloader_clotho, 31, 'test', True, device, 'Clotho')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ceb9c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if table_num == 2 or setting_num == 3 :\n",
    "    metrics_audiocaps, captions_pred_audiocaps, captions_gt_audiocaps = eval_model(model, test_dataloader_audiocaps, 31, 'test', True, device, 'AudioCaps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1903be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minkyu_env",
   "language": "python",
   "name": "minkyu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
