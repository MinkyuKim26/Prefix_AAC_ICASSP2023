{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "326d7090",
   "metadata": {},
   "source": [
    "### 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "881623e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# custom\n",
    "from util import *\n",
    "from transformers import GPT2Tokenizer\n",
    "from AAC_Prefix.AAC_Prefix import * # network\n",
    "from Train import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43b6e3e",
   "metadata": {},
   "source": [
    "### 기타 값들 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f11fd5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefix vector 크기 설정\n",
    "temporal_prefix_size = 15 # 0 or 15\n",
    "global_prefix_size = 11 # 0 or 11\n",
    "\n",
    "prefix_size = temporal_prefix_size + global_prefix_size \n",
    "\n",
    "# mapping network가 사용할 transformer의 스펙 설정\n",
    "transformer_num_layers = {\"temporal_num_layers\" : 4, \"global_num_layers\" : 4}\n",
    "prefix_size_dict = {\"temporal_prefix_size\" : temporal_prefix_size, \"global_prefix_size\" : global_prefix_size}\n",
    "\n",
    "\n",
    "data_dir = './AudioCaps'\n",
    "MODEL_NAME = 'add_exp_train_audiocaps_test_clotho'\n",
    "\n",
    "epochs = 50\n",
    "LR = 5e-5\n",
    "\n",
    "TEST_BATCH_SIZE = 5\n",
    "TRAIN_BATCH_SIZE = 75\n",
    "\n",
    "random_seed=2766\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "torch.backends.cudnn.benchmark=False\n",
    "torch.backends.cudnn.deterministic=True\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)  \n",
    "\n",
    "USE_CUDA = torch.cuda.is_available() \n",
    "device = torch.device('cuda' if USE_CUDA else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858fd3cb",
   "metadata": {},
   "source": [
    "### Tokenizer, Dataloader 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e644b5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_type = 'GPT2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "vocab_size = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ee36a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "get dataset...: 100%|████████████████████| 49276/49276 [05:48<00:00, 141.19it/s]\n",
      "get dataset...: 100%|██████████████████████| 1045/1045 [00:06<00:00, 167.92it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = CreateDataloader(tokenizer, data_dir, TRAIN_BATCH_SIZE, 'train', prefix_size, is_TrainDataset = True, tokenizer_type = tokenizer_type)\n",
    "test_dataloader_clotho = CreateDataloader(tokenizer, './Clotho', TEST_BATCH_SIZE, 'evaluation', prefix_size, is_TrainDataset = False, tokenizer_type = tokenizer_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a5974b",
   "metadata": {},
   "source": [
    "### 학습결과 정리하는 폴더 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adeff9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"./Train_record/params_\" + MODEL_NAME\n",
    "try:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "except OSError:\n",
    "    print(\"Error: Failed to create the directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac319136",
   "metadata": {},
   "source": [
    "### 모델 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31a202ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cuai5th/anaconda3/envs/minkyu_env/lib/python3.8/site-packages/torchlibrosa/stft.py:193: FutureWarning: Pass size=512 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  fft_window = librosa.util.pad_center(fft_window, n_fft)\n",
      "/home/cuai5th/anaconda3/envs/minkyu_env/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use GPT2 Tokenizer\n",
      "temporal feature's mapping network : num_head = 8 num_layers = 4\n",
      "global feature ver's mapping network : num_head = 8 num_layers = 4\n",
      "Encoder freezing\n",
      "GPT2 freezing\n"
     ]
    }
   ],
   "source": [
    "model = get_AAC_Prefix(tokenizer, \n",
    "                        vocab_size = vocab_size, Dataset = 'AudioCaps',\n",
    "                        prefix_size_dict = prefix_size_dict, transformer_num_layers = transformer_num_layers, \n",
    "                        encoder_freeze = True, decoder_freeze = True,\n",
    "                        pretrain_fromAudioCaps = False, device = device)\n",
    "\n",
    "# 다른 데이터셋으로 평가하는데 이 때 조건이 Header는 freeze하는 조건이 들어있었다. 그래서 Header freeze 해줌\n",
    "for param in model.language_header.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca1860f",
   "metadata": {},
   "source": [
    "### 학습 & 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6c61f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cuai5th/anaconda3/envs/minkyu_env/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Training Epoch 0, Loss = 5.10774: 100%|███████| 656/656 [06:03<00:00,  1.80it/s]\n",
      "Training Epoch 1, Loss = 3.62106: 100%|███████| 656/656 [06:02<00:00,  1.81it/s]\n",
      "Training Epoch 2, Loss = 3.40983: 100%|███████| 656/656 [06:02<00:00,  1.81it/s]\n",
      "Training Epoch 3, Loss = 3.2746: 100%|████████| 656/656 [06:03<00:00,  1.81it/s]\n",
      "Training Epoch 4, Loss = 3.17474: 100%|███████| 656/656 [06:02<00:00,  1.81it/s]\n",
      "Training Epoch 5, Loss = 3.08877: 100%|███████| 656/656 [06:02<00:00,  1.81it/s]\n",
      "Training Epoch 6, Loss = 3.011: 100%|█████████| 656/656 [06:02<00:00,  1.81it/s]\n",
      "Training Epoch 7, Loss = 2.94504: 100%|███████| 656/656 [06:02<00:00,  1.81it/s]\n",
      "Training Epoch 8, Loss = 2.89053: 100%|███████| 656/656 [06:02<00:00,  1.81it/s]\n",
      "Training Epoch 9, Loss = 2.84444: 100%|███████| 656/656 [06:02<00:00,  1.81it/s]\n",
      "Training Epoch 10, Loss = 2.81199: 100%|██████| 656/656 [06:02<00:00,  1.81it/s]\n",
      "Training Epoch 11, Loss = 2.78059: 100%|██████| 656/656 [06:01<00:00,  1.81it/s]\n",
      "Training Epoch 12, Loss = 2.75748: 100%|██████| 656/656 [06:02<00:00,  1.81it/s]\n",
      "Training Epoch 13, Loss = 2.73463: 100%|██████| 656/656 [06:02<00:00,  1.81it/s]\n",
      "Training Epoch 14, Loss = 2.71337: 100%|██████| 656/656 [06:02<00:00,  1.81it/s]\n",
      "Eval using dataset...: 100%|████████████████| 1045/1045 [04:06<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.007327\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 70696 tokens at 473341.46 tokens per second.\n",
      "PTBTokenizer tokenized 10386 tokens at 120137.35 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 8176, 'reflen': 9775, 'guess': [8176, 7131, 6086, 5049], 'correct': [3504, 945, 280, 58]}\n",
      "ratio: 0.8364194373400679\n",
      "Bleu_1: 0.352\n",
      "Bleu_2: 0.196\n",
      "Bleu_3: 0.113\n",
      "Bleu_4: 0.061\n",
      "computing METEOR score...\n",
      "METEOR: 0.109\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.277\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.168\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) [17.334 seconds]\n",
      "Error: Could not cache item to /home/cuai5th/CUAI_2022/MinkyuKim/AAC_Project_2022-main/coco_caption/pycocoevalcap/spice/cache with key:\n",
      "\"a continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous\"\n",
      "Caption may be too long\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 24.06 s\n",
      "SPICE: 0.072\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15, Loss = 2.69751: 100%|██████| 656/656 [06:02<00:00,  1.81it/s]\n",
      "Training Epoch 16, Loss = 2.68052: 100%|██████| 656/656 [06:02<00:00,  1.81it/s]\n",
      "Training Epoch 17, Loss = 2.66704: 100%|██████| 656/656 [06:02<00:00,  1.81it/s]\n",
      "Training Epoch 18, Loss = 2.65518: 100%|██████| 656/656 [06:01<00:00,  1.81it/s]\n",
      "Training Epoch 19, Loss = 2.64263: 100%|██████| 656/656 [06:02<00:00,  1.81it/s]\n",
      "Eval using dataset...: 100%|████████████████| 1045/1045 [03:49<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.006793\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 70696 tokens at 488725.27 tokens per second.\n",
      "PTBTokenizer tokenized 9707 tokens at 109979.82 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 7515, 'reflen': 9698, 'guess': [7515, 6470, 5425, 4404], 'correct': [3383, 912, 258, 49]}\n",
      "ratio: 0.7749020416579939\n",
      "Bleu_1: 0.337\n",
      "Bleu_2: 0.188\n",
      "Bleu_3: 0.108\n",
      "Bleu_4: 0.057\n",
      "computing METEOR score...\n",
      "METEOR: 0.107\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.271\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.161\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) [12.816 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 19.23 s\n",
      "SPICE: 0.069\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20, Loss = 2.63131: 100%|██████| 656/656 [06:03<00:00,  1.80it/s]\n",
      "Training Epoch 21, Loss = 2.62036: 100%|██████| 656/656 [06:03<00:00,  1.80it/s]\n",
      "Training Epoch 22, Loss = 2.61046: 100%|██████| 656/656 [06:03<00:00,  1.80it/s]\n",
      "Training Epoch 23, Loss = 2.60181: 100%|██████| 656/656 [06:03<00:00,  1.81it/s]\n",
      "Training Epoch 24, Loss = 2.59172: 100%|██████| 656/656 [06:02<00:00,  1.81it/s]\n",
      "Eval using dataset...: 100%|████████████████| 1045/1045 [04:04<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.006848\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 70696 tokens at 478265.15 tokens per second.\n",
      "PTBTokenizer tokenized 10195 tokens at 107891.10 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 8041, 'reflen': 9773, 'guess': [8041, 6996, 5951, 4927], 'correct': [3420, 865, 245, 52]}\n",
      "ratio: 0.8227770387802289\n",
      "Bleu_1: 0.343\n",
      "Bleu_2: 0.185\n",
      "Bleu_3: 0.104\n",
      "Bleu_4: 0.056\n",
      "computing METEOR score...\n",
      "METEOR: 0.109\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.272\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.164\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) [13.572 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 20.20 s\n",
      "SPICE: 0.070\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 25, Loss = 2.58345: 100%|██████| 656/656 [06:02<00:00,  1.81it/s]\n",
      "Training Epoch 26, Loss = 2.57749: 100%|██████| 656/656 [06:02<00:00,  1.81it/s]\n",
      "Training Epoch 27, Loss = 2.57071: 100%|██████| 656/656 [06:02<00:00,  1.81it/s]\n",
      "Training Epoch 28, Loss = 2.56113: 100%|██████| 656/656 [06:02<00:00,  1.81it/s]\n",
      "Training Epoch 29, Loss = 2.555: 100%|████████| 656/656 [06:02<00:00,  1.81it/s]\n",
      "Eval using dataset...: 100%|████████████████| 1045/1045 [03:35<00:00,  4.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.006699\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 70696 tokens at 490735.33 tokens per second.\n",
      "PTBTokenizer tokenized 9082 tokens at 108259.52 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 6961, 'reflen': 9606, 'guess': [6961, 5916, 4871, 3846], 'correct': [3337, 889, 269, 67]}\n",
      "ratio: 0.7246512596293229\n",
      "Bleu_1: 0.328\n",
      "Bleu_2: 0.184\n",
      "Bleu_3: 0.108\n",
      "Bleu_4: 0.062\n",
      "computing METEOR score...\n",
      "METEOR: 0.108\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.278\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.179\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) [9.622 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 16.02 s\n",
      "SPICE: 0.070\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 30, Loss = 2.54849: 100%|██████| 656/656 [06:02<00:00,  1.81it/s]\n",
      "Training Epoch 31, Loss = 2.54236: 100%|██████| 656/656 [06:02<00:00,  1.81it/s]\n",
      "Training Epoch 32, Loss = 2.53626: 100%|██████| 656/656 [06:03<00:00,  1.81it/s]\n",
      "Training Epoch 33, Loss = 2.53412: 100%|██████| 656/656 [06:02<00:00,  1.81it/s]\n",
      "Training Epoch 34, Loss = 2.52509: 100%|██████| 656/656 [06:02<00:00,  1.81it/s]\n",
      "Eval using dataset...: 100%|████████████████| 1045/1045 [03:43<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.007007\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 70696 tokens at 475292.98 tokens per second.\n",
      "PTBTokenizer tokenized 9675 tokens at 107980.96 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 7444, 'reflen': 9736, 'guess': [7444, 6399, 5354, 4320], 'correct': [3476, 929, 269, 55]}\n",
      "ratio: 0.7645850451930192\n",
      "Bleu_1: 0.343\n",
      "Bleu_2: 0.191\n",
      "Bleu_3: 0.111\n",
      "Bleu_4: 0.060\n",
      "computing METEOR score...\n",
      "METEOR: 0.111\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.279\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.179\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) [6.505 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 14.06 s\n",
      "SPICE: 0.070\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 35, Loss = 2.52201: 100%|██████| 656/656 [06:02<00:00,  1.81it/s]\n",
      "Training Epoch 36, Loss = 2.51763: 100%|██████| 656/656 [06:02<00:00,  1.81it/s]\n",
      "Training Epoch 37, Loss = 2.51408: 100%|██████| 656/656 [06:02<00:00,  1.81it/s]\n",
      "Training Epoch 38, Loss = 2.51245: 100%|██████| 656/656 [06:02<00:00,  1.81it/s]\n",
      "Training Epoch 39, Loss = 2.50662: 100%|██████| 656/656 [06:02<00:00,  1.81it/s]\n",
      "Eval using dataset...: 100%|████████████████| 1045/1045 [03:48<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.006871\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 70696 tokens at 486139.78 tokens per second.\n",
      "PTBTokenizer tokenized 10035 tokens at 116918.19 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 7758, 'reflen': 9784, 'guess': [7758, 6713, 5668, 4637], 'correct': [3458, 947, 280, 65]}\n",
      "ratio: 0.7929272281274741\n",
      "Bleu_1: 0.343\n",
      "Bleu_2: 0.193\n",
      "Bleu_3: 0.112\n",
      "Bleu_4: 0.063\n",
      "computing METEOR score...\n",
      "METEOR: 0.112\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.276\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.182\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) [11.127 seconds]\n",
      "Error: Could not cache item to /home/cuai5th/CUAI_2022/MinkyuKim/AAC_Project_2022-main/coco_caption/pycocoevalcap/spice/cache with key:\n",
      "\"a continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous\"\n",
      "Caption may be too long\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 17.56 s\n",
      "SPICE: 0.073\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 40, Loss = 2.50497: 100%|██████| 656/656 [06:01<00:00,  1.81it/s]\n",
      "Training Epoch 41, Loss = 2.50078: 100%|██████| 656/656 [06:02<00:00,  1.81it/s]\n",
      "Training Epoch 42, Loss = 2.4978: 100%|███████| 656/656 [06:02<00:00,  1.81it/s]\n",
      "Training Epoch 43, Loss = 2.49517: 100%|██████| 656/656 [06:02<00:00,  1.81it/s]\n",
      "Training Epoch 44, Loss = 2.49512: 100%|██████| 656/656 [06:02<00:00,  1.81it/s]\n",
      "Eval using dataset...: 100%|████████████████| 1045/1045 [03:50<00:00,  4.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.006741\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 70696 tokens at 451766.18 tokens per second.\n",
      "PTBTokenizer tokenized 9944 tokens at 115794.65 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 7663, 'reflen': 9709, 'guess': [7663, 6618, 5573, 4539], 'correct': [3405, 916, 271, 56]}\n",
      "ratio: 0.7892676897722948\n",
      "Bleu_1: 0.340\n",
      "Bleu_2: 0.190\n",
      "Bleu_3: 0.110\n",
      "Bleu_4: 0.060\n",
      "computing METEOR score...\n",
      "METEOR: 0.111\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.273\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.180\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) [9.311 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 15.91 s\n",
      "SPICE: 0.073\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 45, Loss = 2.49576: 100%|██████| 656/656 [06:02<00:00,  1.81it/s]\n",
      "Training Epoch 46, Loss = 2.49364: 100%|██████| 656/656 [06:02<00:00,  1.81it/s]\n",
      "Training Epoch 47, Loss = 2.49303: 100%|██████| 656/656 [06:02<00:00,  1.81it/s]\n",
      "Training Epoch 48, Loss = 2.49299: 100%|██████| 656/656 [06:02<00:00,  1.81it/s]\n",
      "Training Epoch 49, Loss = 2.49272: 100%|██████| 656/656 [06:02<00:00,  1.81it/s]\n",
      "Eval using dataset...: 100%|████████████████| 1045/1045 [03:51<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.006988\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 70696 tokens at 496100.36 tokens per second.\n",
      "PTBTokenizer tokenized 10007 tokens at 109871.30 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 7747, 'reflen': 9717, 'guess': [7747, 6702, 5657, 4622], 'correct': [3432, 924, 273, 58]}\n",
      "ratio: 0.7972625295872391\n",
      "Bleu_1: 0.344\n",
      "Bleu_2: 0.192\n",
      "Bleu_3: 0.111\n",
      "Bleu_4: 0.060\n",
      "computing METEOR score...\n",
      "METEOR: 0.111\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.274\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.181\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) [10.691 seconds]\n",
      "Error: Could not cache item to /home/cuai5th/CUAI_2022/MinkyuKim/AAC_Project_2022-main/coco_caption/pycocoevalcap/spice/cache with key:\n",
      "\"a continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous continuous\"\n",
      "Caption may be too long\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 17.11 s\n",
      "SPICE: 0.073\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.127\n",
      "\n",
      "Training time : 5:02:13\n"
     ]
    }
   ],
   "source": [
    "Train(model, LR, train_dataloader, test_dataloader_clotho,\n",
    "    epochs, model_name = MODEL_NAME, beam_search = True, device = device,\n",
    "    Dataset = 'AudioCaps', test_dataloader_other_dataset = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cee443f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minkyu_env",
   "language": "python",
   "name": "minkyu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
