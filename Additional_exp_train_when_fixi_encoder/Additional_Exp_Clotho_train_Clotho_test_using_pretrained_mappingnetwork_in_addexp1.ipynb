{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "326d7090",
   "metadata": {},
   "source": [
    "### 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "881623e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "\n",
    "# custom\n",
    "from util import *\n",
    "from transformers import GPT2Tokenizer\n",
    "from AAC_Prefix.AAC_Prefix import * # network\n",
    "from Train import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43b6e3e",
   "metadata": {},
   "source": [
    "### 기타 값들 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f11fd5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefix vector 크기 설정\n",
    "temporal_prefix_size = 15 # 0 or 15\n",
    "global_prefix_size = 11 # 0 or 11\n",
    "\n",
    "prefix_size = temporal_prefix_size + global_prefix_size \n",
    "\n",
    "# mapping network가 사용할 transformer의 스펙 설정\n",
    "transformer_num_layers = {\"temporal_num_layers\" : 4, \"global_num_layers\" : 4}\n",
    "prefix_size_dict = {\"temporal_prefix_size\" : temporal_prefix_size, \"global_prefix_size\" : global_prefix_size}\n",
    "\n",
    "\n",
    "data_dir = './Clotho'\n",
    "MODEL_NAME = 'add_exp_train_clotho_test_clotho_using_pretrained_mappingnetwork_in_addexp1'\n",
    "\n",
    "epochs = 60\n",
    "LR = 5e-5\n",
    "\n",
    "TEST_BATCH_SIZE = 5\n",
    "TRAIN_BATCH_SIZE = 55\n",
    "\n",
    "random_seed=2766\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "torch.backends.cudnn.benchmark=False\n",
    "torch.backends.cudnn.deterministic=True\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)  \n",
    "\n",
    "USE_CUDA = torch.cuda.is_available() \n",
    "device = torch.device('cuda' if USE_CUDA else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858fd3cb",
   "metadata": {},
   "source": [
    "### Tokenizer, Dataloader 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e644b5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_type = 'Custom'\n",
    "tokenizer = tokenizer_forCustomVocab(Dataset = 'Clotho')\n",
    "vocab_size = len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ee36a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "get dataset...: 100%|██████████████████████| 1045/1045 [00:06<00:00, 171.62it/s]\n",
      "get dataset...: 100%|██████████████████████| 2893/2893 [00:22<00:00, 128.14it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataloader  = CreateDataloader(tokenizer, data_dir, TEST_BATCH_SIZE, 'evaluation', prefix_size, is_TrainDataset = False, tokenizer_type = tokenizer_type)\n",
    "train_dataloader = CreateDataloader(tokenizer, data_dir, TRAIN_BATCH_SIZE, 'development', prefix_size, is_TrainDataset = True, tokenizer_type = tokenizer_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a5974b",
   "metadata": {},
   "source": [
    "### 학습결과 정리하는 폴더 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adeff9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"./Train_record/params_\" + MODEL_NAME\n",
    "try:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "except OSError:\n",
    "    print(\"Error: Failed to create the directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac319136",
   "metadata": {},
   "source": [
    "### 모델 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31a202ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cuai5th/anaconda3/envs/minkyu_env/lib/python3.8/site-packages/torchlibrosa/stft.py:193: FutureWarning: Pass size=512 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  fft_window = librosa.util.pad_center(fft_window, n_fft)\n",
      "/home/cuai5th/anaconda3/envs/minkyu_env/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use Custom Tokenizer\n",
      "temporal feature's mapping network : num_head = 8 num_layers = 4\n",
      "global feature ver's mapping network : num_head = 8 num_layers = 4\n",
      "use custom header!\n",
      "Encoder freezing\n",
      "GPT2 freezing\n",
      "use Custom Tokenizer\n",
      "temporal feature's mapping network : num_head = 8 num_layers = 4\n",
      "global feature ver's mapping network : num_head = 8 num_layers = 4\n",
      "use custom header!\n",
      "Encoder freezing\n",
      "GPT2 freezing\n"
     ]
    }
   ],
   "source": [
    "model = get_AAC_Prefix(tokenizer, \n",
    "                        vocab_size = vocab_size, Dataset = 'Clotho',\n",
    "                        prefix_size_dict = prefix_size_dict, transformer_num_layers = transformer_num_layers, \n",
    "                        encoder_freeze = True, decoder_freeze = True,\n",
    "                        pretrain_fromAudioCaps = False, device = device)\n",
    "\n",
    "# pre_trained된 mapping network가져와야함\n",
    "tokenizer_addexp1 = tokenizer_forCustomVocab(Dataset = 'AudioCaps')\n",
    "vocab_size_addexp1 = len(tokenizer_addexp1.vocab)\n",
    "\n",
    "\n",
    "model_in_addexp1 = get_AAC_Prefix(tokenizer_addexp1, \n",
    "                        vocab_size = vocab_size_addexp1, Dataset = 'AudioCaps',\n",
    "                        prefix_size_dict = prefix_size_dict, transformer_num_layers = transformer_num_layers, \n",
    "                        encoder_freeze = True, decoder_freeze = True,\n",
    "                        pretrain_fromAudioCaps = False, device = device)\n",
    "\n",
    "params_path = './Train_record/params_add_exp_train_audiocaps_test_audiocaps/Param_epoch_39.pt'\n",
    "params = torch.load(params_path, map_location = device)\n",
    "model_in_addexp1.load_state_dict(params) \n",
    "\n",
    "model.temporal_mappingnetwork = copy.deepcopy(model_in_addexp1.temporal_mappingnetwork)\n",
    "model.global_mappingnetwork = copy.deepcopy(model_in_addexp1.global_mappingnetwork)\n",
    "\n",
    "del model_in_addexp1, params_path, params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca1860f",
   "metadata": {},
   "source": [
    "### 학습 & 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6c61f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cuai5th/anaconda3/envs/minkyu_env/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Training Epoch 0, Loss = 9.81329: 100%|███████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 1, Loss = 7.5022: 100%|████████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 2, Loss = 6.37142: 100%|███████| 263/263 [02:11<00:00,  1.99it/s]\n",
      "Training Epoch 3, Loss = 5.94813: 100%|███████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 4, Loss = 5.73951: 100%|███████| 263/263 [02:11<00:00,  1.99it/s]\n",
      "Training Epoch 5, Loss = 5.58739: 100%|███████| 263/263 [02:11<00:00,  1.99it/s]\n",
      "Training Epoch 6, Loss = 5.45246: 100%|███████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 7, Loss = 5.31796: 100%|███████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 8, Loss = 5.19508: 100%|███████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 9, Loss = 5.07989: 100%|███████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 10, Loss = 4.97338: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 11, Loss = 4.87528: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 12, Loss = 4.78839: 100%|██████| 263/263 [02:13<00:00,  1.97it/s]\n",
      "Training Epoch 13, Loss = 4.70611: 100%|██████| 263/263 [02:13<00:00,  1.98it/s]\n",
      "Training Epoch 14, Loss = 4.63291: 100%|██████| 263/263 [02:13<00:00,  1.98it/s]\n",
      "Eval using dataset...: 100%|████████████████| 1045/1045 [03:56<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.007305\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 70696 tokens at 475937.67 tokens per second.\n",
      "PTBTokenizer tokenized 12148 tokens at 135035.94 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 10059, 'reflen': 10491, 'guess': [10059, 9014, 7969, 6924], 'correct': [5343, 1966, 666, 139]}\n",
      "ratio: 0.9588218472975923\n",
      "Bleu_1: 0.509\n",
      "Bleu_2: 0.326\n",
      "Bleu_3: 0.204\n",
      "Bleu_4: 0.113\n",
      "computing METEOR score...\n",
      "METEOR: 0.146\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.351\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.269\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) [7.712 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 15.62 s\n",
      "SPICE: 0.096\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15, Loss = 4.56456: 100%|██████| 263/263 [02:13<00:00,  1.98it/s]\n",
      "Training Epoch 16, Loss = 4.50369: 100%|██████| 263/263 [02:12<00:00,  1.98it/s]\n",
      "Training Epoch 17, Loss = 4.44653: 100%|██████| 263/263 [02:12<00:00,  1.98it/s]\n",
      "Training Epoch 18, Loss = 4.38927: 100%|██████| 263/263 [02:12<00:00,  1.98it/s]\n",
      "Training Epoch 19, Loss = 4.33408: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Eval using dataset...: 100%|████████████████| 1045/1045 [03:36<00:00,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.006748\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 70696 tokens at 486066.51 tokens per second.\n",
      "PTBTokenizer tokenized 11460 tokens at 125942.23 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 9371, 'reflen': 10129, 'guess': [9371, 8326, 7281, 6236], 'correct': [5239, 1980, 690, 141]}\n",
      "ratio: 0.9251653667685926\n",
      "Bleu_1: 0.516\n",
      "Bleu_2: 0.336\n",
      "Bleu_3: 0.215\n",
      "Bleu_4: 0.120\n",
      "computing METEOR score...\n",
      "METEOR: 0.149\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.357\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.270\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) [5.119 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 12.83 s\n",
      "SPICE: 0.097\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20, Loss = 4.28429: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 21, Loss = 4.23411: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 22, Loss = 4.18856: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 23, Loss = 4.14056: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 24, Loss = 4.10568: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Eval using dataset...: 100%|████████████████| 1045/1045 [03:32<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.007060\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 70696 tokens at 495347.04 tokens per second.\n",
      "PTBTokenizer tokenized 11477 tokens at 128336.31 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 9387, 'reflen': 10027, 'guess': [9387, 8342, 7297, 6252], 'correct': [5413, 2084, 735, 172]}\n",
      "ratio: 0.9361723346962265\n",
      "Bleu_1: 0.539\n",
      "Bleu_2: 0.355\n",
      "Bleu_3: 0.228\n",
      "Bleu_4: 0.132\n",
      "computing METEOR score...\n",
      "METEOR: 0.153\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.365\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.315\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) [5.889 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 14.03 s\n",
      "SPICE: 0.103\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 25, Loss = 4.06015: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 26, Loss = 4.024: 100%|████████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 27, Loss = 3.98771: 100%|██████| 263/263 [02:11<00:00,  2.00it/s]\n",
      "Training Epoch 28, Loss = 3.95249: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 29, Loss = 3.91344: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Eval using dataset...: 100%|████████████████| 1045/1045 [03:36<00:00,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.007070\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 70696 tokens at 472252.71 tokens per second.\n",
      "PTBTokenizer tokenized 12141 tokens at 137158.54 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 10050, 'reflen': 10369, 'guess': [10050, 9005, 7960, 6915], 'correct': [5599, 2157, 759, 194]}\n",
      "ratio: 0.9692352203683123\n",
      "Bleu_1: 0.540\n",
      "Bleu_2: 0.354\n",
      "Bleu_3: 0.226\n",
      "Bleu_4: 0.133\n",
      "computing METEOR score...\n",
      "METEOR: 0.156\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.364\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.309\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.5 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) [5.347 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 13.51 s\n",
      "SPICE: 0.107\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.208\n",
      "set encoder freeze!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 30, Loss = 3.87994: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 31, Loss = 3.85167: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 32, Loss = 3.82269: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 33, Loss = 3.79592: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 34, Loss = 3.77466: 100%|██████| 263/263 [02:12<00:00,  1.98it/s]\n",
      "Eval using dataset...: 100%|████████████████| 1045/1045 [03:32<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.006964\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 70696 tokens at 494923.58 tokens per second.\n",
      "PTBTokenizer tokenized 11913 tokens at 137271.94 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 9822, 'reflen': 10280, 'guess': [9822, 8777, 7732, 6687], 'correct': [5555, 2181, 788, 202]}\n",
      "ratio: 0.9554474708170276\n",
      "Bleu_1: 0.540\n",
      "Bleu_2: 0.358\n",
      "Bleu_3: 0.232\n",
      "Bleu_4: 0.138\n",
      "computing METEOR score...\n",
      "METEOR: 0.155\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.365\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.310\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.8 sec].\n",
      "Threads( StanfordCoreNLP ) [4.709 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 12.78 s\n",
      "SPICE: 0.107\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 35, Loss = 3.74321: 100%|██████| 263/263 [02:12<00:00,  1.98it/s]\n",
      "Training Epoch 36, Loss = 3.71708: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 37, Loss = 3.69769: 100%|██████| 263/263 [02:12<00:00,  1.98it/s]\n",
      "Training Epoch 38, Loss = 3.67846: 100%|██████| 263/263 [02:12<00:00,  1.98it/s]\n",
      "Training Epoch 39, Loss = 3.65154: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Eval using dataset...: 100%|████████████████| 1045/1045 [03:31<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.006901\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 70696 tokens at 487057.73 tokens per second.\n",
      "PTBTokenizer tokenized 11394 tokens at 117556.82 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 9304, 'reflen': 10020, 'guess': [9304, 8259, 7214, 6169], 'correct': [5451, 2123, 753, 186]}\n",
      "ratio: 0.928542914171564\n",
      "Bleu_1: 0.542\n",
      "Bleu_2: 0.359\n",
      "Bleu_3: 0.232\n",
      "Bleu_4: 0.137\n",
      "computing METEOR score...\n",
      "METEOR: 0.155\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.366\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.314\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) [4.390 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 12.57 s\n",
      "SPICE: 0.105\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 40, Loss = 3.63881: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 41, Loss = 3.61975: 100%|██████| 263/263 [02:12<00:00,  1.98it/s]\n",
      "Training Epoch 42, Loss = 3.60858: 100%|██████| 263/263 [02:12<00:00,  1.98it/s]\n",
      "Training Epoch 43, Loss = 3.58904: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 44, Loss = 3.57912: 100%|██████| 263/263 [02:11<00:00,  1.99it/s]\n",
      "Eval using dataset...: 100%|████████████████| 1045/1045 [03:50<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.006924\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 70696 tokens at 490468.19 tokens per second.\n",
      "PTBTokenizer tokenized 11720 tokens at 133483.25 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 9629, 'reflen': 10160, 'guess': [9629, 8584, 7539, 6494], 'correct': [5490, 2149, 786, 207]}\n",
      "ratio: 0.9477362204723476\n",
      "Bleu_1: 0.540\n",
      "Bleu_2: 0.358\n",
      "Bleu_3: 0.233\n",
      "Bleu_4: 0.140\n",
      "computing METEOR score...\n",
      "METEOR: 0.154\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.365\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.314\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) [4.641 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 12.77 s\n",
      "SPICE: 0.105\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 45, Loss = 3.56876: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 46, Loss = 3.55325: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 47, Loss = 3.5391: 100%|███████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 48, Loss = 3.53463: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 49, Loss = 3.52778: 100%|██████| 263/263 [02:11<00:00,  1.99it/s]\n",
      "Eval using dataset...: 100%|████████████████| 1045/1045 [03:37<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.007069\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 70696 tokens at 488349.84 tokens per second.\n",
      "PTBTokenizer tokenized 12039 tokens at 136609.57 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 9946, 'reflen': 10318, 'guess': [9946, 8901, 7856, 6811], 'correct': [5567, 2171, 788, 204]}\n",
      "ratio: 0.9639465012598406\n",
      "Bleu_1: 0.539\n",
      "Bleu_2: 0.356\n",
      "Bleu_3: 0.230\n",
      "Bleu_4: 0.137\n",
      "computing METEOR score...\n",
      "METEOR: 0.155\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.365\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.314\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) [4.468 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 12.63 s\n",
      "SPICE: 0.107\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 50, Loss = 3.52042: 100%|██████| 263/263 [02:11<00:00,  2.00it/s]\n",
      "Training Epoch 51, Loss = 3.51762: 100%|██████| 263/263 [02:11<00:00,  2.00it/s]\n",
      "Training Epoch 52, Loss = 3.51642: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 53, Loss = 3.51094: 100%|██████| 263/263 [02:11<00:00,  2.00it/s]\n",
      "Training Epoch 54, Loss = 3.50929: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Eval using dataset...: 100%|████████████████| 1045/1045 [03:34<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.006839\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 70696 tokens at 466909.47 tokens per second.\n",
      "PTBTokenizer tokenized 11833 tokens at 134141.60 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 9738, 'reflen': 10191, 'guess': [9738, 8693, 7648, 6603], 'correct': [5548, 2181, 788, 211]}\n",
      "ratio: 0.9555490138356436\n",
      "Bleu_1: 0.544\n",
      "Bleu_2: 0.361\n",
      "Bleu_3: 0.234\n",
      "Bleu_4: 0.141\n",
      "computing METEOR score...\n",
      "METEOR: 0.156\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.367\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.315\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.5 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) [3.885 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 11.83 s\n",
      "SPICE: 0.107\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 55, Loss = 3.50178: 100%|██████| 263/263 [02:11<00:00,  1.99it/s]\n",
      "Training Epoch 56, Loss = 3.49882: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 57, Loss = 3.50041: 100%|██████| 263/263 [02:12<00:00,  1.98it/s]\n",
      "Training Epoch 58, Loss = 3.49704: 100%|██████| 263/263 [02:12<00:00,  1.98it/s]\n",
      "Training Epoch 59, Loss = 3.50386: 100%|██████| 263/263 [02:12<00:00,  1.98it/s]\n",
      "Eval using dataset...: 100%|████████████████| 1045/1045 [03:34<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.006816\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 70696 tokens at 466463.27 tokens per second.\n",
      "PTBTokenizer tokenized 11905 tokens at 134815.06 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 9813, 'reflen': 10254, 'guess': [9813, 8768, 7723, 6678], 'correct': [5551, 2173, 793, 214]}\n",
      "ratio: 0.9569923932123116\n",
      "Bleu_1: 0.541\n",
      "Bleu_2: 0.358\n",
      "Bleu_3: 0.233\n",
      "Bleu_4: 0.140\n",
      "computing METEOR score...\n",
      "METEOR: 0.156\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.366\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.316\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) [2.694 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 10.71 s\n",
      "SPICE: 0.106\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.211\n",
      "\n",
      "Training time : 2:12:19\n"
     ]
    }
   ],
   "source": [
    "Train(model, LR, train_dataloader, test_dataloader,\n",
    "    epochs, model_name = MODEL_NAME, beam_search = True, device = device,\n",
    "    Dataset = 'Clotho', test_dataloader_other_dataset = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cee443f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minkyu_env",
   "language": "python",
   "name": "minkyu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
