{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "326d7090",
   "metadata": {},
   "source": [
    "### 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "881623e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "\n",
    "# custom\n",
    "from util import *\n",
    "from transformers import GPT2Tokenizer\n",
    "from AAC_Prefix.AAC_Prefix import * # network\n",
    "from Train import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43b6e3e",
   "metadata": {},
   "source": [
    "### 기타 값들 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f11fd5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefix vector 크기 설정\n",
    "temporal_prefix_size = 15 # 0 or 15\n",
    "global_prefix_size = 11 # 0 or 11\n",
    "\n",
    "prefix_size = temporal_prefix_size + global_prefix_size \n",
    "\n",
    "# mapping network가 사용할 transformer의 스펙 설정\n",
    "transformer_num_layers = {\"temporal_num_layers\" : 4, \"global_num_layers\" : 4}\n",
    "prefix_size_dict = {\"temporal_prefix_size\" : temporal_prefix_size, \"global_prefix_size\" : global_prefix_size}\n",
    "\n",
    "\n",
    "data_dir = './Clotho'\n",
    "MODEL_NAME = 'add_exp_train_clotho_test_clotho'\n",
    "\n",
    "epochs = 60\n",
    "LR = 5e-5\n",
    "\n",
    "TEST_BATCH_SIZE = 5\n",
    "TRAIN_BATCH_SIZE = 55\n",
    "\n",
    "random_seed=2766\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "torch.backends.cudnn.benchmark=False\n",
    "torch.backends.cudnn.deterministic=True\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)  \n",
    "\n",
    "USE_CUDA = torch.cuda.is_available() \n",
    "device = torch.device('cuda' if USE_CUDA else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858fd3cb",
   "metadata": {},
   "source": [
    "### Tokenizer, Dataloader 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e644b5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_type = 'Custom'\n",
    "tokenizer = tokenizer_forCustomVocab(Dataset = 'Clotho')\n",
    "vocab_size = len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ee36a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "get dataset...: 100%|██████████████████████| 1045/1045 [00:05<00:00, 176.01it/s]\n",
      "get dataset...: 100%|██████████████████████| 2893/2893 [00:22<00:00, 127.43it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataloader  = CreateDataloader(tokenizer, data_dir, TEST_BATCH_SIZE, 'evaluation', prefix_size, is_TrainDataset = False, tokenizer_type = tokenizer_type)\n",
    "train_dataloader = CreateDataloader(tokenizer, data_dir, TRAIN_BATCH_SIZE, 'development', prefix_size, is_TrainDataset = True, tokenizer_type = tokenizer_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a5974b",
   "metadata": {},
   "source": [
    "### 학습결과 정리하는 폴더 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adeff9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"./Train_record/params_\" + MODEL_NAME\n",
    "try:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "except OSError:\n",
    "    print(\"Error: Failed to create the directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac319136",
   "metadata": {},
   "source": [
    "### 모델 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31a202ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cuai5th/anaconda3/envs/minkyu_env/lib/python3.8/site-packages/torchlibrosa/stft.py:193: FutureWarning: Pass size=512 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  fft_window = librosa.util.pad_center(fft_window, n_fft)\n",
      "/home/cuai5th/anaconda3/envs/minkyu_env/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use Custom Tokenizer\n",
      "temporal feature's mapping network : num_head = 8 num_layers = 4\n",
      "global feature ver's mapping network : num_head = 8 num_layers = 4\n",
      "use custom header!\n",
      "Encoder freezing\n",
      "GPT2 freezing\n"
     ]
    }
   ],
   "source": [
    "model = get_AAC_Prefix(tokenizer, \n",
    "                        vocab_size = vocab_size, Dataset = 'Clotho',\n",
    "                        prefix_size_dict = prefix_size_dict, transformer_num_layers = transformer_num_layers, \n",
    "                        encoder_freeze = True, decoder_freeze = True,\n",
    "                        pretrain_fromAudioCaps = False, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca1860f",
   "metadata": {},
   "source": [
    "### 학습 & 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6c61f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cuai5th/anaconda3/envs/minkyu_env/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Training Epoch 0, Loss = 14.87104: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 1, Loss = 9.49262: 100%|███████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 2, Loss = 8.35977: 100%|███████| 263/263 [02:11<00:00,  2.00it/s]\n",
      "Training Epoch 3, Loss = 6.73762: 100%|███████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 4, Loss = 6.21935: 100%|███████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 5, Loss = 6.03939: 100%|███████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 6, Loss = 5.92525: 100%|███████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 7, Loss = 5.83836: 100%|███████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 8, Loss = 5.76258: 100%|███████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 9, Loss = 5.69483: 100%|███████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 10, Loss = 5.6239: 100%|███████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 11, Loss = 5.5489: 100%|███████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 12, Loss = 5.47934: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 13, Loss = 5.40219: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 14, Loss = 5.33194: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Eval using dataset...: 100%|████████████████| 1045/1045 [04:49<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.007221\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 70696 tokens at 471479.45 tokens per second.\n",
      "PTBTokenizer tokenized 12590 tokens at 140667.60 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 10501, 'reflen': 10758, 'guess': [10501, 9456, 8411, 7366], 'correct': [3482, 732, 132, 9]}\n",
      "ratio: 0.9761108012640847\n",
      "Bleu_1: 0.324\n",
      "Bleu_2: 0.156\n",
      "Bleu_3: 0.072\n",
      "Bleu_4: 0.026\n",
      "computing METEOR score...\n",
      "METEOR: 0.094\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.256\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.070\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.5 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) [11.101 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 19.03 s\n",
      "SPICE: 0.079\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15, Loss = 5.26651: 100%|██████| 263/263 [02:13<00:00,  1.97it/s]\n",
      "Training Epoch 16, Loss = 5.2034: 100%|███████| 263/263 [02:12<00:00,  1.98it/s]\n",
      "Training Epoch 17, Loss = 5.14224: 100%|██████| 263/263 [02:12<00:00,  1.98it/s]\n",
      "Training Epoch 18, Loss = 5.08099: 100%|██████| 263/263 [02:12<00:00,  1.98it/s]\n",
      "Training Epoch 19, Loss = 5.01813: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Eval using dataset...: 100%|████████████████| 1045/1045 [04:19<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.006592\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 70696 tokens at 488491.61 tokens per second.\n",
      "PTBTokenizer tokenized 12965 tokens at 145454.48 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 10876, 'reflen': 10909, 'guess': [10876, 9831, 8786, 7741], 'correct': [4260, 1246, 295, 39]}\n",
      "ratio: 0.9969749747913652\n",
      "Bleu_1: 0.391\n",
      "Bleu_2: 0.222\n",
      "Bleu_3: 0.118\n",
      "Bleu_4: 0.054\n",
      "computing METEOR score...\n",
      "METEOR: 0.112\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.294\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.128\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) [7.596 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 15.25 s\n",
      "SPICE: 0.086\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20, Loss = 4.96223: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 21, Loss = 4.91214: 100%|██████| 263/263 [02:12<00:00,  1.98it/s]\n",
      "Training Epoch 22, Loss = 4.85691: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 23, Loss = 4.80733: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 24, Loss = 4.76477: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Eval using dataset...: 100%|████████████████| 1045/1045 [04:17<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.006718\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 70696 tokens at 473980.70 tokens per second.\n",
      "PTBTokenizer tokenized 12198 tokens at 141082.82 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 10109, 'reflen': 10555, 'guess': [10109, 9064, 8019, 6974], 'correct': [4678, 1518, 478, 97]}\n",
      "ratio: 0.9577451444811977\n",
      "Bleu_1: 0.443\n",
      "Bleu_2: 0.266\n",
      "Bleu_3: 0.159\n",
      "Bleu_4: 0.086\n",
      "computing METEOR score...\n",
      "METEOR: 0.126\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.315\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.183\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) [6.984 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 15.30 s\n",
      "SPICE: 0.092\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 25, Loss = 4.72399: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 26, Loss = 4.67962: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 27, Loss = 4.6404: 100%|███████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 28, Loss = 4.59463: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 29, Loss = 4.55799: 100%|██████| 263/263 [02:12<00:00,  1.98it/s]\n",
      "Eval using dataset...: 100%|████████████████| 1045/1045 [04:04<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.007107\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 70696 tokens at 489875.33 tokens per second.\n",
      "PTBTokenizer tokenized 12043 tokens at 135823.06 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 9954, 'reflen': 10398, 'guess': [9954, 8909, 7864, 6819], 'correct': [5129, 1795, 591, 135]}\n",
      "ratio: 0.9572994806692674\n",
      "Bleu_1: 0.493\n",
      "Bleu_2: 0.308\n",
      "Bleu_3: 0.190\n",
      "Bleu_4: 0.107\n",
      "computing METEOR score...\n",
      "METEOR: 0.138\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.337\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.231\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) [5.881 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 13.59 s\n",
      "SPICE: 0.098\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.165\n",
      "set encoder freeze!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 30, Loss = 4.52172: 100%|██████| 263/263 [02:11<00:00,  1.99it/s]\n",
      "Training Epoch 31, Loss = 4.47666: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 32, Loss = 4.44522: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 33, Loss = 4.40647: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 34, Loss = 4.37142: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Eval using dataset...: 100%|████████████████| 1045/1045 [03:37<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.007075\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 70696 tokens at 486497.20 tokens per second.\n",
      "PTBTokenizer tokenized 11020 tokens at 127194.76 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 8930, 'reflen': 9880, 'guess': [8930, 7885, 6840, 5795], 'correct': [4960, 1785, 590, 150]}\n",
      "ratio: 0.9038461538460624\n",
      "Bleu_1: 0.499\n",
      "Bleu_2: 0.319\n",
      "Bleu_3: 0.199\n",
      "Bleu_4: 0.116\n",
      "computing METEOR score...\n",
      "METEOR: 0.142\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.349\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.262\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) [5.97 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 12.91 s\n",
      "SPICE: 0.102\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 35, Loss = 4.3353: 100%|███████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 36, Loss = 4.30431: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 37, Loss = 4.27396: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 38, Loss = 4.24761: 100%|██████| 263/263 [02:12<00:00,  1.98it/s]\n",
      "Training Epoch 39, Loss = 4.22324: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Eval using dataset...: 100%|████████████████| 1045/1045 [03:36<00:00,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.006993\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 70696 tokens at 494325.08 tokens per second.\n",
      "PTBTokenizer tokenized 11321 tokens at 128606.52 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 9232, 'reflen': 10030, 'guess': [9232, 8187, 7142, 6097], 'correct': [5066, 1904, 668, 166]}\n",
      "ratio: 0.9204386839480637\n",
      "Bleu_1: 0.503\n",
      "Bleu_2: 0.328\n",
      "Bleu_3: 0.210\n",
      "Bleu_4: 0.123\n",
      "computing METEOR score...\n",
      "METEOR: 0.145\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.354\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.269\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) [4.646 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 12.90 s\n",
      "SPICE: 0.098\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 40, Loss = 4.19593: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 41, Loss = 4.17425: 100%|██████| 263/263 [02:11<00:00,  2.00it/s]\n",
      "Training Epoch 42, Loss = 4.15292: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 43, Loss = 4.13457: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 44, Loss = 4.12083: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Eval using dataset...: 100%|████████████████| 1045/1045 [04:10<00:00,  4.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.007230\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 70696 tokens at 488880.72 tokens per second.\n",
      "PTBTokenizer tokenized 12012 tokens at 136216.70 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 9922, 'reflen': 10387, 'guess': [9922, 8877, 7832, 6787], 'correct': [5314, 1974, 691, 185]}\n",
      "ratio: 0.9552325021660772\n",
      "Bleu_1: 0.511\n",
      "Bleu_2: 0.329\n",
      "Bleu_3: 0.209\n",
      "Bleu_4: 0.124\n",
      "computing METEOR score...\n",
      "METEOR: 0.147\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.354\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.271\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) [5.591 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 13.33 s\n",
      "SPICE: 0.100\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 45, Loss = 4.10104: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 46, Loss = 4.08698: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 47, Loss = 4.07754: 100%|██████| 263/263 [02:11<00:00,  1.99it/s]\n",
      "Training Epoch 48, Loss = 4.0656: 100%|███████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 49, Loss = 4.05471: 100%|██████| 263/263 [02:11<00:00,  1.99it/s]\n",
      "Eval using dataset...: 100%|████████████████| 1045/1045 [03:39<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.007012\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 70696 tokens at 473233.10 tokens per second.\n",
      "PTBTokenizer tokenized 11763 tokens at 122272.09 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 9672, 'reflen': 10229, 'guess': [9672, 8627, 7582, 6537], 'correct': [5281, 1989, 703, 178]}\n",
      "ratio: 0.9455469742886943\n",
      "Bleu_1: 0.515\n",
      "Bleu_2: 0.335\n",
      "Bleu_3: 0.214\n",
      "Bleu_4: 0.126\n",
      "computing METEOR score...\n",
      "METEOR: 0.148\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.356\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.280\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) [4.463 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 12.58 s\n",
      "SPICE: 0.101\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 50, Loss = 4.0468: 100%|███████| 263/263 [02:11<00:00,  1.99it/s]\n",
      "Training Epoch 51, Loss = 4.04249: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 52, Loss = 4.03948: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 53, Loss = 4.02976: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Training Epoch 54, Loss = 4.0235: 100%|███████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Eval using dataset...: 100%|████████████████| 1045/1045 [03:41<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.006659\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 70696 tokens at 495441.25 tokens per second.\n",
      "PTBTokenizer tokenized 11867 tokens at 136209.33 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 9776, 'reflen': 10274, 'guess': [9776, 8731, 7686, 6641], 'correct': [5280, 1963, 693, 175]}\n",
      "ratio: 0.9515281292582293\n",
      "Bleu_1: 0.513\n",
      "Bleu_2: 0.331\n",
      "Bleu_3: 0.211\n",
      "Bleu_4: 0.124\n",
      "computing METEOR score...\n",
      "METEOR: 0.148\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.356\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.278\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.5 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) [4.491 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 12.41 s\n",
      "SPICE: 0.101\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 55, Loss = 4.02215: 100%|██████| 263/263 [02:11<00:00,  2.00it/s]\n",
      "Training Epoch 56, Loss = 4.01612: 100%|██████| 263/263 [02:11<00:00,  1.99it/s]\n",
      "Training Epoch 57, Loss = 4.02049: 100%|██████| 263/263 [02:11<00:00,  1.99it/s]\n",
      "Training Epoch 58, Loss = 4.02014: 100%|██████| 263/263 [02:11<00:00,  1.99it/s]\n",
      "Training Epoch 59, Loss = 4.01911: 100%|██████| 263/263 [02:12<00:00,  1.99it/s]\n",
      "Eval using dataset...: 100%|████████████████| 1045/1045 [03:41<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.007144\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 70696 tokens at 495413.74 tokens per second.\n",
      "PTBTokenizer tokenized 11984 tokens at 130126.62 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 9893, 'reflen': 10331, 'guess': [9893, 8848, 7803, 6758], 'correct': [5336, 1956, 696, 176]}\n",
      "ratio: 0.957603329784052\n",
      "Bleu_1: 0.516\n",
      "Bleu_2: 0.330\n",
      "Bleu_3: 0.210\n",
      "Bleu_4: 0.123\n",
      "computing METEOR score...\n",
      "METEOR: 0.148\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.356\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.273\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) [3.390 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 10.99 s\n",
      "SPICE: 0.101\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.187\n",
      "\n",
      "Training time : 2:12:15\n"
     ]
    }
   ],
   "source": [
    "Train(model, LR, train_dataloader, test_dataloader,\n",
    "    epochs, model_name = MODEL_NAME, beam_search = True, device = device,\n",
    "    Dataset = 'Clotho', test_dataloader_other_dataset = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cee443f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minkyu_env",
   "language": "python",
   "name": "minkyu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
