{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "326d7090",
   "metadata": {},
   "source": [
    "### 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "881623e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "\n",
    "# custom\n",
    "from util import *\n",
    "from transformers import GPT2Tokenizer\n",
    "from AAC_Prefix.AAC_Prefix import * # network\n",
    "from Train import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43b6e3e",
   "metadata": {},
   "source": [
    "### 기타 값들 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f11fd5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefix vector 크기 설정\n",
    "temporal_prefix_size = 15 # 0 or 15\n",
    "global_prefix_size = 11 # 0 or 11\n",
    "\n",
    "prefix_size = temporal_prefix_size + global_prefix_size \n",
    "\n",
    "# mapping network가 사용할 transformer의 스펙 설정\n",
    "transformer_num_layers = {\"temporal_num_layers\" : 4, \"global_num_layers\" : 4}\n",
    "prefix_size_dict = {\"temporal_prefix_size\" : temporal_prefix_size, \"global_prefix_size\" : global_prefix_size}\n",
    "\n",
    "\n",
    "data_dir = './Clotho'\n",
    "MODEL_NAME = 'add_exp_train_clotho_test_audiocaps'\n",
    "\n",
    "epochs = 60\n",
    "LR = 5e-5\n",
    "\n",
    "TEST_BATCH_SIZE = 5\n",
    "TRAIN_BATCH_SIZE = 55\n",
    "\n",
    "random_seed=2766\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "torch.backends.cudnn.benchmark=False\n",
    "torch.backends.cudnn.deterministic=True\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)  \n",
    "\n",
    "USE_CUDA = torch.cuda.is_available() \n",
    "device = torch.device('cuda' if USE_CUDA else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858fd3cb",
   "metadata": {},
   "source": [
    "### Tokenizer, Dataloader 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e644b5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_type = 'GPT2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "vocab_size = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ee36a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "get dataset...: 100%|███████████████████████| 2893/2893 [00:56<00:00, 51.39it/s]\n",
      "get dataset...: 100%|███████████████████████| 960/960 [00:00<00:00, 1495.88it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = CreateDataloader(tokenizer, data_dir, TRAIN_BATCH_SIZE, 'development', prefix_size, is_TrainDataset = True, tokenizer_type = tokenizer_type)\n",
    "test_dataloader_audiocaps = CreateDataloader(tokenizer, './AudioCaps', TEST_BATCH_SIZE, 'test', prefix_size, is_TrainDataset = False, tokenizer_type = tokenizer_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a5974b",
   "metadata": {},
   "source": [
    "### 학습결과 정리하는 폴더 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adeff9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"./Train_record/params_\" + MODEL_NAME\n",
    "try:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "except OSError:\n",
    "    print(\"Error: Failed to create the directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac319136",
   "metadata": {},
   "source": [
    "### 모델 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31a202ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cuai5th/anaconda3/envs/minkyu_env/lib/python3.8/site-packages/torchlibrosa/stft.py:193: FutureWarning: Pass size=512 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  fft_window = librosa.util.pad_center(fft_window, n_fft)\n",
      "/home/cuai5th/anaconda3/envs/minkyu_env/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use GPT2 Tokenizer\n",
      "temporal feature's mapping network : num_head = 8 num_layers = 4\n",
      "global feature ver's mapping network : num_head = 8 num_layers = 4\n",
      "Encoder freezing\n",
      "GPT2 freezing\n",
      "header trainable!\n"
     ]
    }
   ],
   "source": [
    "model = get_AAC_Prefix(tokenizer, \n",
    "                        vocab_size = vocab_size, Dataset = 'Clotho',\n",
    "                        prefix_size_dict = prefix_size_dict, transformer_num_layers = transformer_num_layers, \n",
    "                        encoder_freeze = True, decoder_freeze = True,\n",
    "                        pretrain_fromAudioCaps = False, device = device)\n",
    "\n",
    "# 다른 데이터셋으로 평가하는데 이 때 조건이 Header는 freeze하는 조건이 들어있었다. 그래서 Header freeze 해줌\n",
    "for param in model.language_header.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca1860f",
   "metadata": {},
   "source": [
    "### 학습 & 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6c61f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cuai5th/anaconda3/envs/minkyu_env/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Training Epoch 0, Loss = 5.62156: 100%|███████| 263/263 [03:53<00:00,  1.13it/s]\n",
      "Training Epoch 1, Loss = 3.9628: 100%|████████| 263/263 [03:52<00:00,  1.13it/s]\n",
      "Training Epoch 2, Loss = 3.69099: 100%|███████| 263/263 [03:51<00:00,  1.14it/s]\n",
      "Training Epoch 3, Loss = 3.56709: 100%|███████| 263/263 [03:51<00:00,  1.14it/s]\n",
      "Training Epoch 4, Loss = 3.46687: 100%|███████| 263/263 [03:51<00:00,  1.13it/s]\n",
      "Training Epoch 5, Loss = 3.396: 100%|█████████| 263/263 [03:52<00:00,  1.13it/s]\n",
      "Training Epoch 6, Loss = 3.34051: 100%|███████| 263/263 [03:49<00:00,  1.15it/s]\n",
      "Training Epoch 7, Loss = 3.29782: 100%|███████| 263/263 [03:51<00:00,  1.14it/s]\n",
      "Training Epoch 8, Loss = 3.25949: 100%|███████| 263/263 [03:52<00:00,  1.13it/s]\n",
      "Training Epoch 9, Loss = 3.22336: 100%|███████| 263/263 [03:53<00:00,  1.13it/s]\n",
      "Training Epoch 10, Loss = 3.18943: 100%|██████| 263/263 [03:51<00:00,  1.14it/s]\n",
      "Training Epoch 11, Loss = 3.15415: 100%|██████| 263/263 [03:50<00:00,  1.14it/s]\n",
      "Training Epoch 12, Loss = 3.1243: 100%|███████| 263/263 [03:52<00:00,  1.13it/s]\n",
      "Training Epoch 13, Loss = 3.09334: 100%|██████| 263/263 [03:51<00:00,  1.14it/s]\n",
      "Training Epoch 14, Loss = 3.06797: 100%|██████| 263/263 [06:03<00:00,  1.38s/it]\n",
      "Eval using dataset...: 100%|██████████████████| 957/957 [18:44<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.006744\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 59528 tokens at 394543.44 tokens per second.\n",
      "PTBTokenizer tokenized 11257 tokens at 123190.14 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 9331, 'reflen': 8422, 'guess': [9331, 8374, 7417, 6460], 'correct': [4032, 1281, 411, 93]}\n",
      "ratio: 1.107931607694003\n",
      "Bleu_1: 0.432\n",
      "Bleu_2: 0.257\n",
      "Bleu_3: 0.154\n",
      "Bleu_4: 0.085\n",
      "computing METEOR score...\n",
      "METEOR: 0.141\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.333\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.211\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 2.240 s\n",
      "SPICE: 0.080\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15, Loss = 3.04196: 100%|██████| 263/263 [06:36<00:00,  1.51s/it]\n",
      "Training Epoch 16, Loss = 3.01616: 100%|██████| 263/263 [06:01<00:00,  1.38s/it]\n",
      "Training Epoch 17, Loss = 2.99205: 100%|██████| 263/263 [06:40<00:00,  1.52s/it]\n",
      "Training Epoch 18, Loss = 2.96767: 100%|██████| 263/263 [06:42<00:00,  1.53s/it]\n",
      "Training Epoch 19, Loss = 2.94846: 100%|██████| 263/263 [06:42<00:00,  1.53s/it]\n",
      "Eval using dataset...: 100%|██████████████████| 957/957 [18:47<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.006024\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 59528 tokens at 413542.13 tokens per second.\n",
      "PTBTokenizer tokenized 11405 tokens at 107407.27 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 9396, 'reflen': 8624, 'guess': [9396, 8439, 7482, 6525], 'correct': [4038, 1299, 421, 100]}\n",
      "ratio: 1.0895176252317846\n",
      "Bleu_1: 0.430\n",
      "Bleu_2: 0.257\n",
      "Bleu_3: 0.155\n",
      "Bleu_4: 0.087\n",
      "computing METEOR score...\n",
      "METEOR: 0.142\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.321\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.201\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 2.410 s\n",
      "SPICE: 0.082\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20, Loss = 2.92326: 100%|██████| 263/263 [06:40<00:00,  1.52s/it]\n",
      "Training Epoch 21, Loss = 2.90163: 100%|██████| 263/263 [06:43<00:00,  1.53s/it]\n",
      "Training Epoch 22, Loss = 2.87872: 100%|██████| 263/263 [06:41<00:00,  1.53s/it]\n",
      "Training Epoch 23, Loss = 2.86066: 100%|██████| 263/263 [06:43<00:00,  1.54s/it]\n",
      "Training Epoch 24, Loss = 2.83874: 100%|██████| 263/263 [04:00<00:00,  1.09it/s]\n",
      "Eval using dataset...: 100%|██████████████████| 957/957 [13:13<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.006668\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 59528 tokens at 382633.80 tokens per second.\n",
      "PTBTokenizer tokenized 11429 tokens at 116907.15 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 9459, 'reflen': 8691, 'guess': [9459, 8502, 7545, 6588], 'correct': [4098, 1348, 434, 90]}\n",
      "ratio: 1.0883672764927985\n",
      "Bleu_1: 0.433\n",
      "Bleu_2: 0.262\n",
      "Bleu_3: 0.158\n",
      "Bleu_4: 0.086\n",
      "computing METEOR score...\n",
      "METEOR: 0.143\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.324\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.198\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 2.301 s\n",
      "SPICE: 0.087\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 25, Loss = 2.81651: 100%|██████| 263/263 [05:07<00:00,  1.17s/it]\n",
      "Training Epoch 26, Loss = 2.79972: 100%|██████| 263/263 [06:43<00:00,  1.53s/it]\n",
      "Training Epoch 27, Loss = 2.78117: 100%|██████| 263/263 [06:43<00:00,  1.53s/it]\n",
      "Training Epoch 28, Loss = 2.76233: 100%|██████| 263/263 [06:40<00:00,  1.52s/it]\n",
      "Training Epoch 29, Loss = 2.74412: 100%|██████| 263/263 [06:43<00:00,  1.53s/it]\n",
      "Eval using dataset...: 100%|██████████████████| 957/957 [17:56<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.010208\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 59528 tokens at 256940.76 tokens per second.\n",
      "PTBTokenizer tokenized 11049 tokens at 73230.93 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 9114, 'reflen': 8508, 'guess': [9114, 8157, 7200, 6243], 'correct': [4151, 1380, 477, 96]}\n",
      "ratio: 1.0712270803947965\n",
      "Bleu_1: 0.455\n",
      "Bleu_2: 0.278\n",
      "Bleu_3: 0.172\n",
      "Bleu_4: 0.094\n",
      "computing METEOR score...\n",
      "METEOR: 0.147\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.337\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.228\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 2.338 s\n",
      "SPICE: 0.090\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 30, Loss = 2.73109: 100%|██████| 263/263 [06:41<00:00,  1.53s/it]\n",
      "Training Epoch 31, Loss = 2.71394: 100%|██████| 263/263 [06:44<00:00,  1.54s/it]\n",
      "Training Epoch 32, Loss = 2.697: 100%|████████| 263/263 [06:42<00:00,  1.53s/it]\n",
      "Training Epoch 33, Loss = 2.68685: 100%|██████| 263/263 [06:42<00:00,  1.53s/it]\n",
      "Training Epoch 34, Loss = 2.67036: 100%|██████| 263/263 [06:40<00:00,  1.52s/it]\n",
      "Eval using dataset...: 100%|██████████████████| 957/957 [08:28<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.006488\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 59528 tokens at 274567.99 tokens per second.\n",
      "PTBTokenizer tokenized 11712 tokens at 96557.42 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 9712, 'reflen': 8921, 'guess': [9712, 8755, 7798, 6841], 'correct': [4257, 1366, 436, 87]}\n",
      "ratio: 1.0886671897768088\n",
      "Bleu_1: 0.438\n",
      "Bleu_2: 0.262\n",
      "Bleu_3: 0.156\n",
      "Bleu_4: 0.084\n",
      "computing METEOR score...\n",
      "METEOR: 0.146\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.327\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.213\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 2.281 s\n",
      "SPICE: 0.090\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 35, Loss = 2.658: 100%|████████| 263/263 [04:56<00:00,  1.13s/it]\n",
      "Training Epoch 36, Loss = 2.64485: 100%|██████| 263/263 [06:29<00:00,  1.48s/it]\n",
      "Training Epoch 37, Loss = 2.62873: 100%|██████| 263/263 [06:30<00:00,  1.48s/it]\n",
      "Training Epoch 38, Loss = 2.62087: 100%|██████| 263/263 [06:30<00:00,  1.48s/it]\n",
      "Training Epoch 39, Loss = 2.60604: 100%|██████| 263/263 [06:29<00:00,  1.48s/it]\n",
      "Eval using dataset...: 100%|██████████████████| 957/957 [16:18<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.006713\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 59528 tokens at 305853.07 tokens per second.\n",
      "PTBTokenizer tokenized 11664 tokens at 100525.63 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 9643, 'reflen': 8872, 'guess': [9643, 8686, 7729, 6772], 'correct': [4285, 1387, 449, 81]}\n",
      "ratio: 1.0869026149683174\n",
      "Bleu_1: 0.444\n",
      "Bleu_2: 0.266\n",
      "Bleu_3: 0.160\n",
      "Bleu_4: 0.084\n",
      "computing METEOR score...\n",
      "METEOR: 0.146\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.328\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.207\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 2.840 s\n",
      "SPICE: 0.091\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 40, Loss = 2.59819: 100%|██████| 263/263 [06:32<00:00,  1.49s/it]\n",
      "Training Epoch 41, Loss = 2.58976: 100%|██████| 263/263 [06:29<00:00,  1.48s/it]\n",
      "Training Epoch 42, Loss = 2.57858: 100%|██████| 263/263 [06:27<00:00,  1.47s/it]\n",
      "Training Epoch 43, Loss = 2.57208: 100%|██████| 263/263 [06:26<00:00,  1.47s/it]\n",
      "Training Epoch 44, Loss = 2.56094: 100%|██████| 263/263 [06:27<00:00,  1.47s/it]\n",
      "Eval using dataset...: 100%|██████████████████| 957/957 [09:55<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.006858\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 59528 tokens at 392647.20 tokens per second.\n",
      "PTBTokenizer tokenized 12009 tokens at 119476.78 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 10003, 'reflen': 9199, 'guess': [10003, 9046, 8089, 7132], 'correct': [4293, 1360, 427, 72]}\n",
      "ratio: 1.0874008044351464\n",
      "Bleu_1: 0.429\n",
      "Bleu_2: 0.254\n",
      "Bleu_3: 0.150\n",
      "Bleu_4: 0.077\n",
      "computing METEOR score...\n",
      "METEOR: 0.145\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.324\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.203\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 1.977 s\n",
      "SPICE: 0.089\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 45, Loss = 2.5532: 100%|███████| 263/263 [03:49<00:00,  1.15it/s]\n",
      "Training Epoch 46, Loss = 2.54996: 100%|██████| 263/263 [03:50<00:00,  1.14it/s]\n",
      "Training Epoch 47, Loss = 2.53911: 100%|██████| 263/263 [03:51<00:00,  1.14it/s]\n",
      "Training Epoch 48, Loss = 2.53706: 100%|██████| 263/263 [03:52<00:00,  1.13it/s]\n",
      "Training Epoch 49, Loss = 2.53075: 100%|██████| 263/263 [03:50<00:00,  1.14it/s]\n",
      "Eval using dataset...: 100%|██████████████████| 957/957 [05:57<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.006490\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 59528 tokens at 319482.57 tokens per second.\n",
      "PTBTokenizer tokenized 11611 tokens at 99204.72 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 9587, 'reflen': 8852, 'guess': [9587, 8630, 7673, 6716], 'correct': [4079, 1289, 419, 69]}\n",
      "ratio: 1.0830320831449296\n",
      "Bleu_1: 0.425\n",
      "Bleu_2: 0.252\n",
      "Bleu_3: 0.151\n",
      "Bleu_4: 0.077\n",
      "computing METEOR score...\n",
      "METEOR: 0.140\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.317\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.191\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 2.115 s\n",
      "SPICE: 0.085\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 50, Loss = 2.52582: 100%|██████| 263/263 [03:49<00:00,  1.15it/s]\n",
      "Training Epoch 51, Loss = 2.52208: 100%|██████| 263/263 [03:51<00:00,  1.14it/s]\n",
      "Training Epoch 52, Loss = 2.51782: 100%|██████| 263/263 [03:49<00:00,  1.14it/s]\n",
      "Training Epoch 53, Loss = 2.51512: 100%|██████| 263/263 [03:50<00:00,  1.14it/s]\n",
      "Training Epoch 54, Loss = 2.50988: 100%|██████| 263/263 [03:49<00:00,  1.15it/s]\n",
      "Eval using dataset...: 100%|██████████████████| 957/957 [05:56<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.006575\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 59528 tokens at 387461.65 tokens per second.\n",
      "PTBTokenizer tokenized 11720 tokens at 123232.32 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 9682, 'reflen': 8940, 'guess': [9682, 8725, 7768, 6811], 'correct': [4231, 1358, 449, 84]}\n",
      "ratio: 1.0829977628634135\n",
      "Bleu_1: 0.437\n",
      "Bleu_2: 0.261\n",
      "Bleu_3: 0.158\n",
      "Bleu_4: 0.083\n",
      "computing METEOR score...\n",
      "METEOR: 0.143\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.323\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.200\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.5 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Threads( StanfordCoreNLP ) [5.382 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 13.34 s\n",
      "SPICE: 0.086\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 55, Loss = 2.51344: 100%|██████| 263/263 [03:51<00:00,  1.14it/s]\n",
      "Training Epoch 56, Loss = 2.509: 100%|████████| 263/263 [03:48<00:00,  1.15it/s]\n",
      "Training Epoch 57, Loss = 2.5107: 100%|███████| 263/263 [03:50<00:00,  1.14it/s]\n",
      "Training Epoch 58, Loss = 2.5093: 100%|███████| 263/263 [03:50<00:00,  1.14it/s]\n",
      "Training Epoch 59, Loss = 2.50891: 100%|██████| 263/263 [03:50<00:00,  1.14it/s]\n",
      "Eval using dataset...: 100%|██████████████████| 957/957 [05:58<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.006645\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 59528 tokens at 421589.26 tokens per second.\n",
      "PTBTokenizer tokenized 11690 tokens at 123862.71 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 9701, 'reflen': 8896, 'guess': [9701, 8744, 7787, 6830], 'correct': [4218, 1342, 435, 82]}\n",
      "ratio: 1.0904901079135465\n",
      "Bleu_1: 0.435\n",
      "Bleu_2: 0.258\n",
      "Bleu_3: 0.155\n",
      "Bleu_4: 0.082\n",
      "computing METEOR score...\n",
      "METEOR: 0.142\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.321\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.195\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.5 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.5 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) [4.757 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 12.85 s\n",
      "SPICE: 0.086\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.141\n",
      "\n",
      "Training time : 5:10:08\n"
     ]
    }
   ],
   "source": [
    "Train(model, LR, train_dataloader, test_dataloader_audiocaps,\n",
    "    epochs, model_name = MODEL_NAME, beam_search = True, device = device,\n",
    "    Dataset = 'Clotho', test_dataloader_other_dataset = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cee443f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minkyu_env",
   "language": "python",
   "name": "minkyu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
