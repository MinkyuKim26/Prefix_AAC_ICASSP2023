{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "326d7090",
   "metadata": {},
   "source": [
    "### 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "881623e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# custom\n",
    "from util import *\n",
    "from transformers import GPT2Tokenizer\n",
    "from AAC_Prefix.AAC_Prefix import * # network\n",
    "from Train import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43b6e3e",
   "metadata": {},
   "source": [
    "### 기타 값들 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f11fd5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefix vector 크기 설정\n",
    "temporal_prefix_size = 15 # 0 or 15\n",
    "global_prefix_size = 11 # 0 or 11\n",
    "\n",
    "prefix_size = temporal_prefix_size + global_prefix_size \n",
    "\n",
    "# mapping network가 사용할 transformer의 스펙 설정\n",
    "transformer_num_layers = {\"temporal_num_layers\" : 4, \"global_num_layers\" : 4}\n",
    "prefix_size_dict = {\"temporal_prefix_size\" : temporal_prefix_size, \"global_prefix_size\" : global_prefix_size}\n",
    "\n",
    "\n",
    "data_dir = './AudioCaps'\n",
    "MODEL_NAME = 'add_exp_train_audiocaps_test_audiocaps'\n",
    "\n",
    "epochs = 50\n",
    "LR = 5e-5\n",
    "\n",
    "TEST_BATCH_SIZE = 5\n",
    "TRAIN_BATCH_SIZE = 75\n",
    "\n",
    "random_seed=2766\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "torch.backends.cudnn.benchmark=False\n",
    "torch.backends.cudnn.deterministic=True\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)  \n",
    "\n",
    "USE_CUDA = torch.cuda.is_available() \n",
    "device = torch.device('cuda' if USE_CUDA else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858fd3cb",
   "metadata": {},
   "source": [
    "### Tokenizer, Dataloader 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e644b5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_type = 'Custom'\n",
    "tokenizer = tokenizer_forCustomVocab(Dataset = 'AudioCaps')\n",
    "vocab_size = len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ee36a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "get dataset...: 100%|███████████████████████| 960/960 [00:00<00:00, 1400.21it/s]\n",
      "get dataset...: 100%|████████████████████| 49276/49276 [03:00<00:00, 272.83it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataloader  = CreateDataloader(tokenizer, data_dir, TEST_BATCH_SIZE, 'test', prefix_size, is_TrainDataset = False, tokenizer_type = tokenizer_type)\n",
    "train_dataloader = CreateDataloader(tokenizer, data_dir, TRAIN_BATCH_SIZE, 'train', prefix_size, is_TrainDataset = True, tokenizer_type = tokenizer_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a5974b",
   "metadata": {},
   "source": [
    "### 학습결과 정리하는 폴더 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adeff9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"./Train_record/params_\" + MODEL_NAME\n",
    "try:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "except OSError:\n",
    "    print(\"Error: Failed to create the directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac319136",
   "metadata": {},
   "source": [
    "### 모델 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31a202ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cuai5th/anaconda3/envs/minkyu_env/lib/python3.8/site-packages/torchlibrosa/stft.py:193: FutureWarning: Pass size=512 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  fft_window = librosa.util.pad_center(fft_window, n_fft)\n",
      "/home/cuai5th/anaconda3/envs/minkyu_env/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use Custom Tokenizer\n",
      "temporal feature's mapping network : num_head = 8 num_layers = 4\n",
      "global feature ver's mapping network : num_head = 8 num_layers = 4\n",
      "use custom header!\n",
      "Encoder freezing\n",
      "GPT2 freezing\n"
     ]
    }
   ],
   "source": [
    "model = get_AAC_Prefix(tokenizer, \n",
    "                        vocab_size = vocab_size, Dataset = 'AudioCaps',\n",
    "                        prefix_size_dict = prefix_size_dict, transformer_num_layers = transformer_num_layers, \n",
    "                        encoder_freeze = True, decoder_freeze = True,\n",
    "                        pretrain_fromAudioCaps = False, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca1860f",
   "metadata": {},
   "source": [
    "### 학습 & 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6c61f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cuai5th/anaconda3/envs/minkyu_env/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Training Epoch 0, Loss = 12.14145: 100%|██████| 656/656 [05:04<00:00,  2.16it/s]\n",
      "Training Epoch 1, Loss = 8.33692: 100%|███████| 656/656 [05:03<00:00,  2.16it/s]\n",
      "Training Epoch 2, Loss = 6.52476: 100%|███████| 656/656 [05:03<00:00,  2.16it/s]\n",
      "Training Epoch 3, Loss = 5.60416: 100%|███████| 656/656 [05:03<00:00,  2.16it/s]\n",
      "Training Epoch 4, Loss = 5.45392: 100%|███████| 656/656 [05:03<00:00,  2.16it/s]\n",
      "Training Epoch 5, Loss = 5.32693: 100%|███████| 656/656 [05:03<00:00,  2.16it/s]\n",
      "Training Epoch 6, Loss = 5.1957: 100%|████████| 656/656 [05:03<00:00,  2.16it/s]\n",
      "Training Epoch 7, Loss = 5.06584: 100%|███████| 656/656 [05:04<00:00,  2.16it/s]\n",
      "Training Epoch 8, Loss = 4.9322: 100%|████████| 656/656 [05:03<00:00,  2.16it/s]\n",
      "Training Epoch 9, Loss = 4.79338: 100%|███████| 656/656 [05:03<00:00,  2.16it/s]\n",
      "Training Epoch 10, Loss = 4.67208: 100%|██████| 656/656 [05:03<00:00,  2.16it/s]\n",
      "Training Epoch 11, Loss = 4.55623: 100%|██████| 656/656 [05:03<00:00,  2.16it/s]\n",
      "Training Epoch 12, Loss = 4.46033: 100%|██████| 656/656 [05:03<00:00,  2.16it/s]\n",
      "Training Epoch 13, Loss = 4.37534: 100%|██████| 656/656 [05:03<00:00,  2.16it/s]\n",
      "Training Epoch 14, Loss = 4.30547: 100%|██████| 656/656 [05:04<00:00,  2.16it/s]\n",
      "Eval using dataset...: 100%|██████████████████| 957/957 [03:46<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.006509\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 59528 tokens at 423690.93 tokens per second.\n",
      "PTBTokenizer tokenized 8691 tokens at 104160.26 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 6778, 'reflen': 7215, 'guess': [6778, 5821, 4868, 3999], 'correct': [3863, 1710, 636, 117]}\n",
      "ratio: 0.9394317394316092\n",
      "Bleu_1: 0.534\n",
      "Bleu_2: 0.384\n",
      "Bleu_3: 0.262\n",
      "Bleu_4: 0.149\n",
      "computing METEOR score...\n",
      "METEOR: 0.170\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.428\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.425\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) [7.539 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 13.88 s\n",
      "SPICE: 0.122\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15, Loss = 4.24011: 100%|██████| 656/656 [05:03<00:00,  2.16it/s]\n",
      "Training Epoch 16, Loss = 4.18198: 100%|██████| 656/656 [05:04<00:00,  2.15it/s]\n",
      "Training Epoch 17, Loss = 4.13129: 100%|██████| 656/656 [05:03<00:00,  2.16it/s]\n",
      "Training Epoch 18, Loss = 4.08131: 100%|██████| 656/656 [05:04<00:00,  2.15it/s]\n",
      "Training Epoch 19, Loss = 4.03766: 100%|██████| 656/656 [05:04<00:00,  2.15it/s]\n",
      "Eval using dataset...: 100%|██████████████████| 957/957 [03:29<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.006214\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 59528 tokens at 438979.19 tokens per second.\n",
      "PTBTokenizer tokenized 8734 tokens at 106200.34 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 6821, 'reflen': 7232, 'guess': [6821, 5864, 4912, 4013], 'correct': [4294, 2020, 828, 221]}\n",
      "ratio: 0.9431692477874801\n",
      "Bleu_1: 0.593\n",
      "Bleu_2: 0.438\n",
      "Bleu_3: 0.312\n",
      "Bleu_4: 0.199\n",
      "computing METEOR score...\n",
      "METEOR: 0.188\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.454\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.494\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) [4.623 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 12.55 s\n",
      "SPICE: 0.133\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20, Loss = 3.99489: 100%|██████| 656/656 [05:04<00:00,  2.16it/s]\n",
      "Training Epoch 21, Loss = 3.95396: 100%|██████| 656/656 [05:04<00:00,  2.16it/s]\n",
      "Training Epoch 22, Loss = 3.9203: 100%|███████| 656/656 [05:04<00:00,  2.16it/s]\n",
      "Training Epoch 23, Loss = 3.88401: 100%|██████| 656/656 [05:04<00:00,  2.16it/s]\n",
      "Training Epoch 24, Loss = 3.85286: 100%|██████| 656/656 [05:04<00:00,  2.16it/s]\n",
      "Eval using dataset...: 100%|██████████████████| 957/957 [03:09<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.006333\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 59528 tokens at 435440.43 tokens per second.\n",
      "PTBTokenizer tokenized 8967 tokens at 106913.66 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 7048, 'reflen': 7292, 'guess': [7048, 6091, 5136, 4199], 'correct': [4562, 2264, 1002, 302]}\n",
      "ratio: 0.9665386725176951\n",
      "Bleu_1: 0.625\n",
      "Bleu_2: 0.474\n",
      "Bleu_3: 0.348\n",
      "Bleu_4: 0.233\n",
      "computing METEOR score...\n",
      "METEOR: 0.206\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.475\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.562\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) [3.994 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 11.86 s\n",
      "SPICE: 0.152\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 25, Loss = 3.82215: 100%|██████| 656/656 [05:03<00:00,  2.16it/s]\n",
      "Training Epoch 26, Loss = 3.79466: 100%|██████| 656/656 [05:03<00:00,  2.16it/s]\n",
      "Training Epoch 27, Loss = 3.76821: 100%|██████| 656/656 [05:03<00:00,  2.16it/s]\n",
      "Training Epoch 28, Loss = 3.74987: 100%|██████| 656/656 [05:04<00:00,  2.16it/s]\n",
      "Training Epoch 29, Loss = 3.72865: 100%|██████| 656/656 [05:04<00:00,  2.16it/s]\n",
      "Eval using dataset...: 100%|██████████████████| 957/957 [03:05<00:00,  5.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.006336\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 59528 tokens at 425442.81 tokens per second.\n",
      "PTBTokenizer tokenized 8667 tokens at 99123.78 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 6738, 'reflen': 7005, 'guess': [6738, 5781, 4824, 3880], 'correct': [4458, 2199, 974, 310]}\n",
      "ratio: 0.9618843683082138\n",
      "Bleu_1: 0.636\n",
      "Bleu_2: 0.482\n",
      "Bleu_3: 0.356\n",
      "Bleu_4: 0.243\n",
      "computing METEOR score...\n",
      "METEOR: 0.207\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.475\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.555\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) [3.805 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 11.67 s\n",
      "SPICE: 0.152\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 30, Loss = 3.71035: 100%|██████| 656/656 [05:03<00:00,  2.16it/s]\n",
      "Training Epoch 31, Loss = 3.68996: 100%|██████| 656/656 [05:03<00:00,  2.16it/s]\n",
      "Training Epoch 32, Loss = 3.67706: 100%|██████| 656/656 [05:03<00:00,  2.16it/s]\n",
      "Training Epoch 33, Loss = 3.66128: 100%|██████| 656/656 [05:05<00:00,  2.15it/s]\n",
      "Training Epoch 34, Loss = 3.64922: 100%|██████| 656/656 [05:04<00:00,  2.16it/s]\n",
      "Eval using dataset...: 100%|██████████████████| 957/957 [03:11<00:00,  4.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.006646\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 59528 tokens at 438137.04 tokens per second.\n",
      "PTBTokenizer tokenized 9017 tokens at 101026.78 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 7090, 'reflen': 7214, 'guess': [7090, 6133, 5176, 4227], 'correct': [4647, 2343, 1070, 354]}\n",
      "ratio: 0.9828112004434456\n",
      "Bleu_1: 0.644\n",
      "Bleu_2: 0.492\n",
      "Bleu_3: 0.366\n",
      "Bleu_4: 0.252\n",
      "computing METEOR score...\n",
      "METEOR: 0.210\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.483\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.584\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.5 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.5 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) [2.837 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 10.89 s\n",
      "SPICE: 0.158\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 35, Loss = 3.63699: 100%|██████| 656/656 [05:03<00:00,  2.16it/s]\n",
      "Training Epoch 36, Loss = 3.6286: 100%|███████| 656/656 [05:03<00:00,  2.16it/s]\n",
      "Training Epoch 37, Loss = 3.61825: 100%|██████| 656/656 [05:03<00:00,  2.16it/s]\n",
      "Training Epoch 38, Loss = 3.60905: 100%|██████| 656/656 [05:03<00:00,  2.16it/s]\n",
      "Training Epoch 39, Loss = 3.60317: 100%|██████| 656/656 [05:03<00:00,  2.16it/s]\n",
      "Eval using dataset...: 100%|██████████████████| 957/957 [03:09<00:00,  5.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.006525\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 59528 tokens at 413823.60 tokens per second.\n",
      "PTBTokenizer tokenized 8831 tokens at 104260.42 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 6890, 'reflen': 7015, 'guess': [6890, 5933, 4976, 4027], 'correct': [4614, 2336, 1082, 367]}\n",
      "ratio: 0.9821810406270873\n",
      "Bleu_1: 0.658\n",
      "Bleu_2: 0.504\n",
      "Bleu_3: 0.379\n",
      "Bleu_4: 0.264\n",
      "computing METEOR score...\n",
      "METEOR: 0.214\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.485\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.602\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) [3.56 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 10.45 s\n",
      "SPICE: 0.163\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 40, Loss = 3.59616: 100%|██████| 656/656 [05:03<00:00,  2.16it/s]\n",
      "Training Epoch 41, Loss = 3.59048: 100%|██████| 656/656 [05:03<00:00,  2.16it/s]\n",
      "Training Epoch 42, Loss = 3.58667: 100%|██████| 656/656 [05:04<00:00,  2.16it/s]\n",
      "Training Epoch 43, Loss = 3.58307: 100%|██████| 656/656 [05:04<00:00,  2.15it/s]\n",
      "Training Epoch 44, Loss = 3.58048: 100%|██████| 656/656 [05:04<00:00,  2.15it/s]\n",
      "Eval using dataset...: 100%|██████████████████| 957/957 [03:11<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.006330\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 59528 tokens at 428421.15 tokens per second.\n",
      "PTBTokenizer tokenized 9045 tokens at 106241.87 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 7098, 'reflen': 7201, 'guess': [7098, 6141, 5184, 4234], 'correct': [4720, 2387, 1120, 372]}\n",
      "ratio: 0.9856964310511059\n",
      "Bleu_1: 0.655\n",
      "Bleu_2: 0.501\n",
      "Bleu_3: 0.377\n",
      "Bleu_4: 0.261\n",
      "computing METEOR score...\n",
      "METEOR: 0.215\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.483\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.598\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) [3.978 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 11.47 s\n",
      "SPICE: 0.166\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 45, Loss = 3.58041: 100%|██████| 656/656 [05:03<00:00,  2.16it/s]\n",
      "Training Epoch 46, Loss = 3.57602: 100%|██████| 656/656 [05:04<00:00,  2.15it/s]\n",
      "Training Epoch 47, Loss = 3.57468: 100%|██████| 656/656 [05:04<00:00,  2.16it/s]\n",
      "Training Epoch 48, Loss = 3.57527: 100%|██████| 656/656 [05:03<00:00,  2.16it/s]\n",
      "Training Epoch 49, Loss = 3.57621: 100%|██████| 656/656 [05:03<00:00,  2.16it/s]\n",
      "Eval using dataset...: 100%|██████████████████| 957/957 [03:10<00:00,  5.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.006377\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 59528 tokens at 431194.68 tokens per second.\n",
      "PTBTokenizer tokenized 8892 tokens at 94572.46 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 6954, 'reflen': 7116, 'guess': [6954, 5997, 5040, 4090], 'correct': [4643, 2359, 1118, 382]}\n",
      "ratio: 0.9772344013489351\n",
      "Bleu_1: 0.652\n",
      "Bleu_2: 0.501\n",
      "Bleu_3: 0.379\n",
      "Bleu_4: 0.265\n",
      "computing METEOR score...\n",
      "METEOR: 0.214\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.484\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.601\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) [2.388 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 9.894 s\n",
      "SPICE: 0.162\n",
      "computing SPIDEr score...\n",
      "SPIDEr: 0.381\n",
      "\n",
      "Training time : 4:13:18\n"
     ]
    }
   ],
   "source": [
    "Train(model, LR, train_dataloader, test_dataloader,\n",
    "    epochs, model_name = MODEL_NAME, beam_search = True, device = device,\n",
    "    Dataset = 'AudioCaps', test_dataloader_other_dataset = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cee443f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minkyu_env",
   "language": "python",
   "name": "minkyu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
